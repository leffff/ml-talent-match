{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1dcbab07-d39c-442d-962c-2caa1bd15db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from joblib import load\n",
    "from copy import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from transformers import AutoModelForTokenClassification, BertTokenizerFast, BertConfig, AutoModel, AutoTokenizer, RobertaTokenizerFast\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65a4d245-1bd7-4a9d-aa78-a3f0ffafac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # Model\n",
    "    model_name = \"ai-forever/ruRoberta-large\"\n",
    "    \n",
    "    # Training\n",
    "    batch_size = 256\n",
    "    epochs = 20\n",
    "    learning_rate = 5e-5\n",
    "    lr_warmup_steps = 500\n",
    "\n",
    "    # Accelerator\n",
    "    gradient_accumulation_steps = 1\n",
    "    mixed_precision = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision\n",
    "\n",
    "    device = \"cuda\"\n",
    "    random_state = 42 \n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c747c67a-9d2e-472e-81a9-f222d299727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int,\n",
    "                    use_deterministic_algos: bool = False) -> None:\n",
    "    \n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(use_deterministic_algos)\n",
    "    random.seed(seed)\n",
    "    \n",
    "\n",
    "seed_everything(config.random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ebed8f90-87ae-4816-80cf-69706a0b19d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForTokenClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    config.model_name,\n",
    "    num_labels=12\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0dcd1c34-457a-4961-973e-fa96a2726627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 798,732 || all params: 355,121,176 || trainable%: 0.22491815582408412\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63deb474-8732-4395-9a26-84bea273f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16104164-d412-4528-a990-a194e81d8df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'relevance', '__index_level_0__'],\n",
       "    num_rows: 2476083\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "df = df.dropna()\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "641eeee2-b15d-450d-aa7d-2dbc94ce5592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 2476083/2476083 [02:54<00:00, 14155.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(\n",
    "        sample['question'], sample['answer'], truncation=True, padding='max_length', max_length=256\n",
    "    ),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dfe8841-41ec-425d-9908-fdc7f1ae6f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question',\n",
       " 'answer',\n",
       " 'relevance',\n",
       " '__index_level_0__',\n",
       " 'input_ids',\n",
       " 'token_type_ids',\n",
       " 'attention_mask']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(encoded_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "626eb7e3-2def-40d3-a64c-471bd0f57a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relevance', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset = encoded_dataset.remove_columns([\n",
    " 'question',\n",
    " 'answer',\n",
    " '__index_level_0__',\n",
    "])\n",
    "\n",
    "list(encoded_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "678b2c32-9cb4-403e-a03a-9c43636bef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(type='torch', columns=['relevance', 'input_ids', 'token_type_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e596c68-4fbc-47ad-afa3-e02877c60e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds, test_inds = train_test_split([i for i in range(len(encoded_dataset))], test_size=0.2)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    Subset(encoded_dataset, train_inds), \n",
    "    batch_size=76,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    Subset(encoded_dataset, test_inds), \n",
    "    batch_size=76,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43dca458-e79f-49d4-9d92-159361aae506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import \n",
    "\n",
    "ROCAUC = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5e552c8-841d-45e2-97cb-b1354d120276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion):\n",
    "    model.train()\n",
    "\n",
    "    all_probas = []\n",
    "    all_labels = []\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids, attention_masks, token_type_ids, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"token_type_ids\"], batch[\"relevance\"]\n",
    "        input_ids, attention_masks, token_type_ids, labels = input_ids.to(DEVICE), attention_masks.to(DEVICE), token_type_ids.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_masks,\n",
    "            token_type_ids=token_type_ids\n",
    "        ).logits\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        accelerator.backward(loss)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        probas = output.softmax(dim=-1)\n",
    "        all_probas.append(probas.detach().cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "        \n",
    "    metrics = {\n",
    "        \"Loss\": total_loss / len(dataloader)\n",
    "    }\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "def val_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    all_probas = []\n",
    "    all_labels = []\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids, attention_masks, token_type_ids, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"token_type_ids\"], batch[\"relevance\"]\n",
    "        input_ids, attention_masks, token_type_ids, labels = input_ids.to(DEVICE), attention_masks.to(DEVICE), token_type_ids.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_masks,\n",
    "                token_type_ids=token_type_ids\n",
    "            ).logits\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        probas = output.softmax(dim=-1)\n",
    "        all_probas.append(probas.detach().cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "        \n",
    "\n",
    "    metrics = {\n",
    "        \"Loss\": total_loss / len(dataloader)\n",
    "    }\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    project_name,\n",
    "    model, \n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    criterion\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, criterion)\n",
    "        test_loss = val_epoch(model, test_dataloader, criterion)\n",
    "\n",
    "        print(train_loss, test_loss)\n",
    "\n",
    "        torch.save(model, f\"{project_name}/{i}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec231209-629b-4db4-8295-aacd2b4943f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26065/26065 [1:56:39<00:00,  3.72it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6517/6517 [10:42<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ROCAUC': tensor(0.7484), 'Loss': 0.5820043456033029} {'ROCAUC': tensor(0.7690), 'Loss': 0.5696628986165735}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26065/26065 [1:57:44<00:00,  3.69it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6517/6517 [10:45<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ROCAUC': tensor(0.7757), 'Loss': 0.5572885225995955} {'ROCAUC': tensor(0.7905), 'Loss': 0.54619039610904}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26065/26065 [1:55:01<00:00,  3.78it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6517/6517 [09:34<00:00, 11.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ROCAUC': tensor(0.8260), 'Loss': 0.5038450323406849} {'ROCAUC': tensor(0.8565), 'Loss': 0.4671186056466525}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26065/26065 [2:00:14<00:00,  3.61it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6517/6517 [10:43<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ROCAUC': tensor(0.8759), 'Loss': 0.43368750158550873} {'ROCAUC': tensor(0.8766), 'Loss': 0.4372068587116107}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███████▉                                                                                                              | 1741/26065 [07:46<1:48:43,  3.73it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m*\u001b[39m epochs)\n\u001b[1;32m      9\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m get_cosine_schedule_with_warmup(optimizer, \n\u001b[1;32m     10\u001b[0m                                     num_warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;66;03m# Default value in run_glue.py\u001b[39;00m\n\u001b[1;32m     11\u001b[0m                                     num_training_steps \u001b[38;5;241m=\u001b[39m total_steps)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 90\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, epochs, train_dataloader, test_dataloader, optimizer, scheduler, criterion)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_loop\u001b[39m(\n\u001b[1;32m     81\u001b[0m     model, \n\u001b[1;32m     82\u001b[0m     epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     criterion\n\u001b[1;32m     88\u001b[0m ):\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 90\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m         test_loss \u001b[38;5;241m=\u001b[39m val_epoch(model, test_dataloader, criterion)\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28mprint\u001b[39m(train_loss, test_loss)\n",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, scheduler, criterion)\u001b[0m\n\u001b[1;32m     26\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 29\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m probas \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/PycharmProjects/af-bot/venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/af-bot/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/af-bot/venv/lib/python3.10/site-packages/madgrad/madgrad.py:201\u001b[0m, in \u001b[0;36mMADGRAD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    198\u001b[0m     z \u001b[38;5;241m=\u001b[39m x0\u001b[38;5;241m.\u001b[39maddcdiv(s, rms, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# p is a moving average of z\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     \u001b[43mp_data_fp32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mck\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(z, alpha\u001b[38;5;241m=\u001b[39mck)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m decouple_decay:\n\u001b[1;32m    204\u001b[0m     p_data_fp32\u001b[38;5;241m.\u001b[39madd_(p_old, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mlr\u001b[38;5;241m*\u001b[39mdecay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from madgrad import MADGRAD\n",
    "\n",
    "epochs = 20\n",
    "optimizer = MADGRAD([\n",
    "        {\"params\": model.parameters(), \"lr\": config.learning_rate},\n",
    "])\n",
    "total_steps = int(len(train_dataloader) * epochs)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                    num_warmup_steps = cofig.lr_warmup_steps, # Default value in run_glue.py\n",
    "                                    num_training_steps = total_steps)\n",
    "\n",
    "train_loop(\n",
    "    project_name=\"\",\n",
    "    model=model, \n",
    "    epochs=epochs,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=val_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7023a4dd-a9f7-4449-bd61-4617806b7b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume\n",
      "resume_id\n",
      "first_name\n",
      "last_name\n",
      "middle_name\n",
      "birth_date\n",
      "birth_date_year_only\n",
      "country\n",
      "city\n",
      "about\n",
      "key_skills\n",
      "salary_expectations_amount\n",
      "salary_expectations_currency\n",
      "photo_path\n",
      "gender\n",
      "language\n",
      "resume_name\n",
      "source_link\n",
      "contactItems\n",
      "resume_contact_item_id\n",
      "value\n",
      "comment\n",
      "contact_type\n",
      "educationItems\n",
      "resume_education_item_id\n",
      "year\n",
      "organization\n",
      "faculty\n",
      "specialty\n",
      "result\n",
      "education_type\n",
      "education_level\n",
      "experienceItems\n",
      "resume_experience_item_id\n",
      "starts\n",
      "ends\n",
      "employer\n",
      "city\n",
      "url\n",
      "position\n",
      "description\n",
      "order\n",
      "languageItems\n",
      "resume_language_item_id\n",
      "language\n",
      "language_level\n",
      "O\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(['resume', 'resume_id', 'first_name', 'last_name', 'middle_name', 'birth_date', 'birth_date_year_only', 'country', 'city', 'about', 'key_skills', 'salary_expectations_amount', 'salary_expectations_currency', 'photo_path', 'gender', 'language', 'resume_name', 'source_link', 'contactItems', 'resume_contact_item_id', 'value', 'comment', 'contact_type', 'educationItems', 'resume_education_item_id', 'year', 'organization', 'faculty', 'specialty', 'result', 'education_type', 'education_level', 'experienceItems', 'resume_experience_item_id', 'starts', 'ends', 'employer', 'city', 'url', 'position', 'description', 'order', 'languageItems', 'resume_language_item_id', 'language', 'language_level', 'O']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37dbdaa-6a46-4a66-96ff-b404bf6fb8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
